{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.构造net\n",
    "\n",
    "## 1.1 构造Block\n",
    "Block类是nn模块里提供的一个模型构造类，简单概括Block的基本功能：\n",
    "+ 接收输入数据作为其前向传播函数的参数。\n",
    "+ 通过使前向传播函数返回一个值来生成输出。请注意，输出的形状可能与输入的形状不同。\n",
    "+ 计算其输出相对于其输入的梯度，可以通过其反向传播函数进行访问。通常，这是自动发生的。\n",
    "+ 存储并提供对执行前向传播计算所需的那些参数。\n",
    "+ 根据需要初始化模型参数。\n",
    "\n",
    "我们可以继承它来定义我们想要的模型：\n",
    "+ 通过重写Block类的__init__方法，构造模型结构\n",
    "+ 通过重写Block类的forward方法，定义参数和正向传播过程。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.06240274, -0.03268593,  0.02582653,  0.02254181, -0.03728798,\n",
       "        -0.04253785,  0.00540612, -0.01364185, -0.09915454, -0.02272737],\n",
       "       [ 0.02816679, -0.03341204,  0.03565665,  0.02506384, -0.04136416,\n",
       "        -0.04941844,  0.01738529,  0.01081963, -0.09932579, -0.01176296]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import npx, np,init\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "\n",
    "X = np.random.uniform(size=(2, 20))\n",
    "\n",
    "class MLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden = nn.Dense(256, activation='relu')  # 定义一个256个隐藏单元的隐藏层\n",
    "        self.out = nn.Dense(10) # 定义一个10纬的输出层\n",
    "        \n",
    "    # 这里的X为输入\n",
    "    def forward(self, X):\n",
    "        return self.out(self.hidden(X))\n",
    "\n",
    "net = MLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  构造Sequential\n",
    "Block类是一个通用的部件。Sequential类继承自Block类。Sequential的作用是当模型的前向计算为简单串联各个层的计算时，可以通过更加简单的方式定义模型。它提供add函数来逐一添加串联的Block子类实例，而模型的forward就是将这些实例按添加的顺序逐一计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySequential(nn.Block):\n",
    "    # 通过add方法将block 添加到_clildren这个字典里\n",
    "    def add(self, block):\n",
    "        self._children[block.name] = block\n",
    "        \n",
    "    # 使用_children中的每一个block处理X\n",
    "    def forward(self, X):\n",
    "        for block in self._children.values():\n",
    "            X=block(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当MySequential调用我们的前向传播函数时，每个添加的块都按照添加的顺序执行。现在，我们可以使用我们的MySequential类来重新实现MLP 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03989595, -0.10414709,  0.06799038,  0.05245074,  0.0252606 ,\n",
       "        -0.00640342,  0.04182098, -0.01665318, -0.02067345, -0.07863816],\n",
       "       [-0.03612847, -0.07210435,  0.09159479,  0.07890773,  0.02494171,\n",
       "        -0.01028665,  0.01732427, -0.02843244,  0.03772651, -0.06671703]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = MySequential()\n",
    "net.add(nn.Dense(256, activation='relu'))\n",
    "net.add(nn.Dense(10))\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 在正向传播中执行代码\n",
    "虽然Sequentail类使模型构造更加简单，需要定义forward函数，但直接继承Block类可以极大地拓展模型构造的灵活性。可以定义自己想要的数学运算，不局限于目前预设好的神经层。可以通过构造自己的block，在正向传播中进行一些计算。\n",
    "构造一个稍微复杂点的网络FancyMLP。在这个网络中，我们通过get_constant函数创建训练中不被迭代的参数，即常数参数。在返回输出之前，我们的模型做了一些不寻常的事情。我们运行了一个while循环，测试它的条件 L1  规范大于  1 ，然后将输出向量除以  2 直到满足条件为止。最后，我们返回中的条目总和X。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FixedHiddenMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.rand_weight=self.params.get_constant('rand_weight', np.random.uniform(size=(20,20)))\n",
    "        self.dense = nn.Dense(20, activation='relu')\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X=self.dense(X)\n",
    "        # 使用创建的常数参数，以及NDArray的relu函数和dot函数\n",
    "        X=npx.relu(np.dot(X, self.rand_weight.data())+1)\n",
    "        # 复用全连接层。等价于两个全连接层共享参数\n",
    "        X= self.dense(X)\n",
    "        # 控制流\n",
    "        while np.abs(X).sum() > 1:\n",
    "            X /= 2\n",
    "        return X.sum()\n",
    "\n",
    "net = FixedHiddenMLP()\n",
    "net.initialize()\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Block混合使用\n",
    "可以讲多个继承于Block的block混合使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.8073602)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NestMLP(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add(nn.Dense(64, activation='relu'),\n",
    "                     nn.Dense(32, activation='relu'))\n",
    "        self.dense = nn.Dense(16, activation='relu')\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.dense(self.net(X))\n",
    "\n",
    "chimera = nn.Sequential()\n",
    "chimera.add(NestMLP(), nn.Dense(20), FixedHiddenMLP())\n",
    "chimera.initialize()\n",
    "chimera(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.参数管理\n",
    "## 2.1 访问模型参数\n",
    "对于Sequential实例中含模型参数的层，我们可以通过Block类的params属性来访问该层包含的所有参数。访问多层感知机net中隐藏层的所有参数。索引0表示隐藏层为Sequential实例最先添加的层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dense10_ (\n",
       "   Parameter dense10_weight (shape=(8, 4), dtype=float32)\n",
       "   Parameter dense10_bias (shape=(8,), dtype=float32)\n",
       " ),\n",
       " mxnet.gluon.parameter.ParameterDict)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "net.add(nn.Dense(8, activation='relu'))\n",
    "net.add(nn.Dense(1))\n",
    "net.initialize()  # 初始化\n",
    "\n",
    "X = np.random.uniform(size=(2, 4))\n",
    "net(X)  \n",
    "\n",
    "net[0].params, type(net[0].params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 参数名称映射到参数实例的字典（类型为ParameterDict类）\n",
    "+ 权重参数的名称为dense10_weight。它由net[0]的名称（dense10_）和自己的变量名（weight）组成\n",
    "+ 该参数的形状为(256, 20)\n",
    "+ 数据类型为32位浮点数（float32）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 访问目标参数\n",
    "访问的方法和访问字典差不多。下代码从第二个神经网络层提取偏差，该偏差返回一个参数类实例，并进一步访问该参数的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mxnet.gluon.parameter.Parameter'>\n",
      "Parameter dense11_bias (shape=(1,), dtype=float32)\n",
      "[0.]\n"
     ]
    }
   ],
   "source": [
    "print(type(net[1].bias))\n",
    "print(net[1].bias)\n",
    "print(net[1].bias.data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[1].weight.grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 访问所有参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense10_ (\n",
      "  Parameter dense10_weight (shape=(8, 4), dtype=float32)\n",
      "  Parameter dense10_bias (shape=(8,), dtype=float32)\n",
      ")\n",
      "sequential2_ (\n",
      "  Parameter dense10_weight (shape=(8, 4), dtype=float32)\n",
      "  Parameter dense10_bias (shape=(8,), dtype=float32)\n",
      "  Parameter dense11_weight (shape=(1, 8), dtype=float32)\n",
      "  Parameter dense11_bias (shape=(1,), dtype=float32)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net[0].collect_params())\n",
    "print(net.collect_params())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 初始化参数\n",
    "默认情况下，MXNet通过从均匀分布中随机绘制来初始化权重参数 $ U(−0.07,0.07)$ ，将偏差参数清除为零。MXNet的init模块提供了多种预设的初始化方法。\n",
    "### 2.2.1 使用内置初始化\n",
    "+ 初始化为0.01高斯数据变量\n",
    "+ force_reinit=True 即便初始化化过同样执行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01142411, 0.01085601, 0.00784212, 0.02014164])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize(init=init.Normal(sigma=0.01), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 使用Constant初始化为常数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33., 33., 33., 33.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize(init=init.Constant(33), force_reinit=True)\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 不同block执行同的初始化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net[0].weight.initialize(init=init.Xavier(), force_reinit=True)\n",
    "net[1].initialize(init=init.Constant(42), force_reinit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2.2 自定义初始化\n",
    "重新实现Initializer类中_init_weight带有张量参数（data）并为其分配所需的初始化值的函数即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init dense10_weight (8, 4)\n",
      "Init dense11_weight (1, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.       , -5.4022765, -6.868823 , -0.       ],\n",
       "       [-6.6433706,  0.       , -5.392142 ,  0.       ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyInit(init.Initializer):\n",
    "    def _init_weight(self, name, data):\n",
    "        print('Init', name, data.shape)\n",
    "        data[:] = np.random.uniform(-10, 10, data.shape)\n",
    "        data *= np.abs(data) >= 5\n",
    "\n",
    "net.initialize(MyInit(), force_reinit=True)\n",
    "net[0].weight.data()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 可以直接修改设置参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42.       , -4.4022765, -5.868823 ,  1.       ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net[0].weight.data()[:] += 1\n",
    "net[0].weight.data()[0, 0] = 42\n",
    "net[0].weight.data()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 捆绑参数\n",
    "我们想跨多个层共享参数。让我们看看如何优雅地做到这一点。接下来，我们分配一个密集层，然后专门使用其参数来设置另一个层的参数。\n",
    "+ 在层之间通过shared标志捆绑\n",
    "+ 在修改了第一层的权重之后，一二层的权重仍然相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True  True  True  True  True  True  True  True]\n",
      "[ True  True  True  True  True  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "net = nn.Sequential()\n",
    "\n",
    "shared = nn.Dense(8, activation='relu')\n",
    "net.add(nn.Dense(8, activation='relu'),\n",
    "        shared,\n",
    "        nn.Dense(8, activation='relu', params=shared.params),\n",
    "        nn.Dense(10))\n",
    "net.initialize()\n",
    "\n",
    "X = np.random.uniform(size=(2, 20))\n",
    "net(X)\n",
    "\n",
    "# 看看参数是否相同\n",
    "print(net[1].weight.data()[0] == net[2].weight.data()[0])\n",
    "net[1].weight.data()[0, 0] = 100\n",
    "# 修改一层的参数，确认参数是否捆绑成功\n",
    "print(net[1].weight.data()[0] == net[2].weight.data()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.延迟初始化\n",
    "前使用Gluon创建的全连接层都没有指定输入个数。例如，在上一节使用的多层感知机net里，我们创建的隐藏层仅仅指定了输出大小为256。当调用initialize函数时，由于隐藏层输入个数依然未知，系统也无法得知该层权重参数的形状。只有在当我们将形状是(2, 20)的输入X传进网络做前向计算net(X)时，系统才推断出该层的权重参数形状为(256, 20)。因此，这时候我们才能真正开始初始化参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "def get_net():\n",
    "    net = nn.Sequential()\n",
    "    net.add(nn.Dense(256, activation='relu'))\n",
    "    net.add(nn.Dense(10))\n",
    "    return net\n",
    "\n",
    "net = get_net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然存在参数对象，但每个图层的输入尺寸都列为-1。MXNet使用特殊值-1表示参数维仍然未知。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential4_ (\n",
       "  Parameter dense16_weight (shape=(256, -1), dtype=float32)\n",
       "  Parameter dense16_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense17_weight (shape=(10, -1), dtype=float32)\n",
       "  Parameter dense17_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.initialize()\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 一旦我们知道输入维数为20，框架就可以通过插入值20来识别第一层的权重矩阵的形状。识别出第一层的形状后，框架将前进至第二层，依此类推计算图形，直到知道所有形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sequential4_ (\n",
       "  Parameter dense16_weight (shape=(256, 20), dtype=float32)\n",
       "  Parameter dense16_bias (shape=(256,), dtype=float32)\n",
       "  Parameter dense17_weight (shape=(10, 256), dtype=float32)\n",
       "  Parameter dense17_bias (shape=(10,), dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.uniform(size=(2, 20))\n",
    "net(X)\n",
    "\n",
    "net.collect_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 自定义层\n",
    "深度学习成功的一个因素是，可以以创造性的方式组成各种层来设计适合各种任务的架构。例如，研究人员发明了专门用于处理图像，文本，遍历顺序数据以及执行动态编程的图层。迟早，您将遇到或发明深度学习框架中尚不存在的层。在这些情况下，您必须构建一个自定义层。\n",
    "## 4.1 无参图层\n",
    "构建一个自定义图层，该图层没有自己的任何参数。只需要继承基层类并实现前向传播函数即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "class CenteredLayer(nn.Block):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 有参图层\n",
    "可以使用内置函数来创建参数，这些参数提供一些基本的内务管理功能。特别是，它们控制访问，初始化，共享，保存和加载模型参数。这样，除其他好处外，我们无需为每个自定义层编写自定义序列化例程。\n",
    "+ 需要两个参数，一个代表重量，另一个代表偏差\n",
    "+ 默认使用ReLU激活激活函数\n",
    "+ 输入参数：in_units和units，分别表示输入和输出的数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(nn.Block):\n",
    "    def __init__(self, units, in_units, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=(in_units, units))\n",
    "        self.bias = self.params.get('bias', shape=(units,))\n",
    "\n",
    "    def forward(self, x):\n",
    "        linear = np.dot(x, self.weight.data(ctx=x.ctx)) + self.bias.data(\n",
    "            ctx=x.ctx)\n",
    "        return npx.relu(linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 模型参数保存\n",
    "在进行长时间的培训时，最佳实践是定期保存中间结果（检查点），以确保在服务器电源线上跳闸时，我们不会损失几天的计算时间。\n",
    "## 5.1 保存加载张量\n",
    "对于单个张量，我们可以直接调用load和save 函数分别读取和写入它们。这两个函数都要求我们提供一个名称，并save要求将要保存的变量作为输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 2., 3.])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "x = np.arange(4)\n",
    "npx.save('x-file', x)\n",
    "\n",
    "x2=npx.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 保存加载模型\n",
    "保存单个权重向量（或其他张量）很有用，但是如果我们要保存（并在以后加载）整个模型，这将非常繁琐。毕竟，我们可能遍布数百个参数组。因此，深度学习框架提供了内置功能来加载和保存整个网络\n",
    "### 5.2.1 Block\n",
    "模型参数的保存和加载， Block 只能保存网络参数\n",
    "\n",
    "load参数：\n",
    "\n",
    "+ allow_missing: True时表示：网络结构中存在, 参数文件中不存在参数，不加载\n",
    "+ ignore_extra: True时表示: 参数文件中存在，网络结构中不存在的参数，不加载\n",
    "+ cast_dtype: True时：将从检查点加载的NDArray的数据类型强制转换为Parameter提供的dtype\n",
    "+ dtype_source: 必须为{‘current’，‘saved’}，仅当cast_dtype = True时有效，指定用于投射参数的dtype的来源\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.save_parameters(\"linear_regression.params\")\n",
    "net.load_parameters(\"linear_regression.params\", ctx=npx.cpu(), allow_missing=False,\n",
    "                    ignore_extra=False, cast_dtype=False, dtype_source='current')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 HybridBlock\n",
    "使用HybridBlock可以同时保存网络结构和参数，通过export导出，import进行加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/mxnet/gluon/block.py:1512: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:\n",
      "\tdata: None\n",
      "  input_sym_arg_type = in_param.infer_type()[0]\n"
     ]
    }
   ],
   "source": [
    "from mxnet.gluon import SymbolBlock\n",
    "\n",
    "net = nn.HybridSequential()\n",
    "net.add(nn.Dense(1))\n",
    "\n",
    "net.initialize(init.Normal(sigma=0.02))\n",
    "net.hybridize()\n",
    "net(X)\n",
    "net.export(\"net1\", epoch=1)\n",
    "\n",
    "\n",
    "net = SymbolBlock.imports(symbol_file='net1-symbol.json', input_names=['data'], param_file='net1-0001.params', \n",
    "                         ctx = npx.cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
